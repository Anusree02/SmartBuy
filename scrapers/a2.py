import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ==== CONFIGURATION ====
search_query      = "laptop"
chromedriver_path = r"C:\Users\mvanu\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
max_pages = 5        # number of search result pages
max_workers = 5      # number of threads for product page scraping

# ==== CHROME DRIVER SETUP ====
chrome_options = Options()
chrome_options.add_argument("--start-maximized")
chrome_service = Service(chromedriver_path)
driver = webdriver.Chrome(service=chrome_service, options=chrome_options)
wait = WebDriverWait(driver, 10)

# ==== SCRAPE SEARCH RESULTS PAGE ====
driver.get("https://www.amazon.in/")
wait.until(EC.presence_of_element_located((By.ID, "twotabsearchtextbox"))).send_keys(search_query + Keys.RETURN)
time.sleep(2)
items = driver.find_elements(By.XPATH, "//div[@data-component-type='s-search-result']")

products = []
prices   = []
links    = []

for page in range(max_pages):
    time.sleep(2)
    items = driver.find_elements(By.XPATH, "//div[@data-component-type='s-search-result']")
    for item in items:
        # Product Name from search result
        try:
            name = item.find_element(By.XPATH, ".//span[@class='a-size-medium a-color-base a-text-normal']").text
        except:
            name = None
        products.append(name)

        # Price from search result
        try:
            price = item.find_element(By.XPATH, ".//span[@class='a-price-whole']").text
        except:
            price = None
        prices.append(price)

        # Product page link
        try:
            link = item.find_element(By.TAG_NAME, "a").get_attribute("href")
        except:
            link = None
        links.append(link)
    try:
        next_btn = driver.find_element(By.XPATH, "//a[contains(@class,'s-pagination-next')]")
        if "disabled" in next_btn.get_attribute("class"):
            break
        driver.execute_script("arguments[0].click();", next_btn)
        time.sleep(2)
    except:
        break

# ==== SCRAPE PRODUCT PAGES FOR DETAILS & Name Fallback ====
discounts      = []
ratings        = []
reviews_counts = []
categories     = []

for idx, link in enumerate(links):
    if not link:
        discounts.append(None)
        ratings.append(None)
        reviews_counts.append(None)
        categories.append(None)
        continue

    driver.get(link)
    wait.until(EC.presence_of_element_located((By.ID, "productTitle")))

    # Fallback product name if missing
    if not products[idx]:
        try:
            products[idx] = driver.find_element(By.ID, "productTitle").text.strip()
        except:
            products[idx] = None

    # Discount
    try:
        discount = driver.find_element(By.CSS_SELECTOR, ".savingsPercentage").text.strip()
    except:
        discount = None
    discounts.append(discount)

    # Ratings
    try:
        ratings.append(driver.find_element(By.ID, "acrPopover").get_attribute("title"))
    except:
        ratings.append(None)

    # Review Count
    try:
        reviews_counts.append(driver.find_element(By.ID, "acrCustomerReviewText").text.strip())
    except:
        reviews_counts.append(None)

    # Category (breadcrumbs)
    try:
        cats = driver.find_elements(By.CSS_SELECTOR, "#wayfinding-breadcrumbs_feature_div ul li span.a-list-item")
        categories.append(" > ".join([c.text.strip() for c in cats if c.text.strip()]))
    except:
        categories.append(None)

driver.quit()

# ==== SAVE TO CSV ====
df = pd.DataFrame({
    "Product": products,
    "Price": prices,
    "Discount": discounts,
    "Ratings": ratings,
    "Reviews Count": reviews_counts,
    "Category": categories,
    "Link": links
})
df.to_csv("amazon_laptop_data.csv", index=False, encoding="utf-8-sig")
print("âœ… Scraping complete! Saved to amazon_laptop_data.csv")
